{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN v0.5: Imporved version of v0.4\n",
    "\n",
    "- generates N training samples with varied energies $K_{\\alpha_1}$, energy splitting and ratios\n",
    "- trains one the MLP regressor for each parameter\n",
    "- The trained NNs are tested on training set, dev set and test set\n",
    "- No optimization have been made.\n",
    "\n",
    "- TODO:\n",
    "    - Split large datasets into several zipped containers\n",
    "\n",
    "Changes:\n",
    "\n",
    "- Removed\n",
    "    - unnecessary fiiting of superposition in gen_set()\n",
    "- New:\n",
    "    - Train separate NN for each parameter but with same training set\n",
    "    - Files now named after size of data set and costum comment\n",
    "    - Convergence control training the NN (but not live yet) \n",
    "- Bug fixes\n",
    "    - Found that paramters also got scaled, fixed it to normal scale ( result was that training was super fast, but had bad results due to too early stopping after few iteration steps ) \n",
    "- gen_set()\n",
    "    - removed fitting of splines to superposition of voigt profiles \n",
    "- read_input_data()\n",
    "    - Now rescales the data matrix correctly without scaling the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import functions and tools needed\n",
    "\"\"\"\n",
    "import plotly.offline as py\n",
    "import plotly.tools as tls\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import math\n",
    "import scipy.stats as sc\n",
    "from scipy.special import wofz\n",
    "from astropy.modeling.functional_models import Voigt1D \n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.neural_network import MLPRegressor as MLPR\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "# Redirect stdout\n",
    "import io \n",
    "import sys\n",
    "# Use subpreocesses\n",
    "import subprocess\n",
    "# Control figure size\n",
    "matplotlib.rcParams['figure.figsize']=(7,5)\n",
    "py.init_notebook_mode()\n",
    "# Use plotly as gifure output\n",
    "def plotly_show():\n",
    "    fig = plt.gcf()\n",
    "    py.iplot_mpl(fig)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_area(x, v):\n",
    "    \"\"\"\n",
    "    Calculates the area beneath a spectrum\n",
    "    \"\"\"\n",
    "    width = (x[len(x)-1]-x[0])/(len(x)-1)\n",
    "    area = 0\n",
    "    for i in range(len(x)):\n",
    "        area += v[i] * width\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_set(N, x, noise, Set, verbosity): \n",
    "    \"\"\"\n",
    "    - Generates a set with N spectra by using the superposition of TWO Voigt profiles with randomly choosen\n",
    "        parameters \n",
    "            gamma1: HWHM of Lorentzian part of Voigt profile 1 \n",
    "            gamma2: HWHM of Lorentzian part of Voigt profile 2\n",
    "            sigma1: Standard uncertainty of Gaussian part of Voigt profile 1\n",
    "            sigma2: Standard uncertainty of Gaussian part of Voigt profile 2\n",
    "            epsilon: offset to energy E\n",
    "        The Energy E (K alpha1) is centered around 2014eV \n",
    "    \"\"\"\n",
    "    if (verbosity > 0):\n",
    "        start = time.time()\n",
    "    # Definition of some parameters\n",
    "    gamma1, sigma1 = 0.345, 0.07\n",
    "    gamma2, sigma2 = 0.36, 0.08\n",
    "    labels = np.array(0)\n",
    "    labels = np.delete(labels, 0)\n",
    "    # Creating the empty data matrix with dimensions N x d+1\n",
    "    X = np.zeros((N,len(x)+3))\n",
    "    runtime = np.array(0)\n",
    "    runtime = np.delete(runtime, 0)\n",
    "    \"\"\"\n",
    "    For loop loops N times to create N spectra. The single spectrum is evaluate and fitted\n",
    "    on range x to get equal x values as features (Note: When trained on grid defined by x then\n",
    "    real data must also be sampled on same grid!). File format:\n",
    "        File dimensions: N x (2 + d), where d is number of grid points resulting from grid x\n",
    "        [E dE x1 x2 ... xd]\n",
    "    \"\"\"\n",
    "    if (verbosity > 0):\n",
    "        loop_time_start = time.time()\n",
    "    for i in range(N):\n",
    "        # Generate random distribution (+- 1) around central value of energie E\n",
    "        E_epsilon = (np.random.random_sample()-0.5)*2\n",
    "        # Generate random distribution (+- 0.1) around central value of energie dE\n",
    "        dE_epsilon = (np.random.random_sample()-0.5)*(0.4)\n",
    "        # Generate random distribution (+- 0.05) around central value of amplitude L1\n",
    "        dL1 = (np.random.random_sample()-0.5)/15\n",
    "        # Generate random distribution (+- 0.05) around central value of amplitude L2\n",
    "        dL2 = (np.random.random_sample()-0.5)/15\n",
    "        L1 = 0.18 + dL1\n",
    "        L2 = 0.3 + dL2\n",
    "        E = 2014 + E_epsilon\n",
    "        dE = 0.85 + dE_epsilon\n",
    "        v1 = Voigt1D(x_0=E-dE, amplitude_L=L1, fwhm_L=2*gamma1, fwhm_G=2*sigma1*np.sqrt(2*np.log(2)))\n",
    "        v2 = Voigt1D(x_0=E, amplitude_L=L2, fwhm_L=2*gamma2, fwhm_G=2*sigma2*np.sqrt(2*np.log(2)))\n",
    "        # Calculate the ratio of the areas\n",
    "        R = calc_area(x, v2(x))/calc_area(x,v1(x))\n",
    "        # Save values in two columns in form of [x,v(x)]\n",
    "        X[i,0] = E\n",
    "        X[i,1] = dE\n",
    "        X[i,2] = R\n",
    "        if (noise == True):\n",
    "            # Apply poisson noise to the data \n",
    "            append = v1(x)+v2(x)\n",
    "            # Magnify amplitudes to get poisson function working\n",
    "            append *= 1e4\n",
    "            append = np.random.poisson(append)\n",
    "            for j in range(len(x)):\n",
    "                X[i,j+3] = append[j]\n",
    "        else: \n",
    "            append = sp(x)\n",
    "            append *= 1e4\n",
    "            for j in range(len(x)):\n",
    "                X[i,j+3] = append[j]\n",
    "        # Runtime control\n",
    "        if (verbosity > 0):\n",
    "            if ( i % (N/10) == 0 ):\n",
    "                loop_time_end = time.time()\n",
    "                time_diff = loop_time_end-loop_time_start\n",
    "                runtime = np.append(runtime, time_diff)\n",
    "                print(\"Progress: %i/%i, time for loop: %3.2fs\" % (i , N, time_diff))\n",
    "                loop_time_start = time.time()\n",
    "    # Create pandas dataframe\n",
    "    df_spectrum = pd.DataFrame(X)\n",
    "    df_spectrum.to_msgpack('./data/'+Set+'.spectrum')\n",
    "    # Plot verbosity information about the loop\n",
    "    if (verbosity > 0):\n",
    "        end = time.time()\n",
    "        print(\"Time for generating the \"+Set+\" set:\", end-start)\n",
    "        if (verbosity > 1):\n",
    "            plt.figure()\n",
    "            plt.title(\"Runtime for generating the data set \"+Set)\n",
    "            plt.plot(runtime, label=\"Runtime per N/10 loops\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plotly_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_set(Set):\n",
    "    \"\"\" \n",
    "    Read data and store it in (Nxd) Martix, where N donates \n",
    "    the observation (single spectrum) and d the dth feature \n",
    "    (datapoint given by choosing x). The data gets fitted \n",
    "    by the Splines fit. Also, noise is added when reading the\n",
    "    data if flag is set.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    X = pd.read_msgpack('./data/'+Set+'.spectrum') \n",
    "    X = X.as_matrix()\n",
    "    # Rescale input between 0 and 1\n",
    "    #X /= 1e4 # factor results of rescaling in gen_set()\n",
    "    X = X.T\n",
    "    for i in range(3, len(X)):\n",
    "        X[i] /= 1e4\n",
    "    X = X.T\n",
    "    end = time.time()\n",
    "    print(\"Time for reading \"+Set+\" set: %3.2fs\" % (end-start))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_input(X):\n",
    "    \"\"\"\n",
    "    Feature skaling for NN apporach. It is \"highly recommended\" to scale input data to either [0:1] or [-1:+1] \n",
    "    or standardize it to have mean 0 and variance 1\n",
    "    Source:\n",
    "    http://scikit-learn.org/stable/modules/neural_networks_supervised.html#regression\n",
    "    This function standardizes X \n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler  \n",
    "    scaler = StandardScaler()  \n",
    "    # Don't cheat - fit only on training data\n",
    "    N = len(X)\n",
    "    d = len(X[0]+3)\n",
    "    y = X[:,:3]\n",
    "    X = X[:,3:]\n",
    "    scaler.fit(X)  \n",
    "    X = scaler.transform(X) \n",
    "    X = np.append(y,X, axis=1)\n",
    "    #for i in range(len(X)):\n",
    "    #    plt.plot(x,X[i])\n",
    "    #plt.grid(True)\n",
    "    #plotly_show()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_train(X, model, parameter, scaling, verbosity):\n",
    "    \"\"\"\n",
    "    Trains given model on data X and labels y. Returns trainings score\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    y = np.ravel(X[:,parameter])\n",
    "    if (scaling == True):\n",
    "        X = scale_input(X)\n",
    "    X = X[:,3:]\n",
    "    # Set out pipe to catch stdout for getting verbosity output of model.fit\n",
    "    if (verbosity > 0):\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = mystdout = io.StringIO()\n",
    "    model.fit(X, y)\n",
    "    # Delete pipe\n",
    "    if (verbosity > 0):\n",
    "        sys.stdout = old_stdout\n",
    "    # Save verbosity output (training loss) in variable\n",
    "    loss = np.array(0)\n",
    "    loss = np.delete(loss, 0)\n",
    "    if (verbosity > 0):\n",
    "        verbosity_output = mystdout.getvalue()\n",
    "        verbosity_output = np.array(verbosity_output.split(' '))\n",
    "        for i in range(4,len(verbosity_output),4):\n",
    "            if (verbosity_output[i].split('\\n')[0] == 'improve'):\n",
    "                break\n",
    "            else:\n",
    "                loss = np.append(loss, float(verbosity_output[i].split('\\n')[0]))\n",
    "    # Save score of training\n",
    "    score = model.score(X,y)\n",
    "    end = time.time()\n",
    "    # Print training statistics depending onb verbosity level\n",
    "    if (verbosity > 0):\n",
    "        print(\"Training time: %3.2f \" % (end - start))\n",
    "        print(\"Training score: %3.2f \" % (score))\n",
    "        if (verbosity > 1):\n",
    "            plt.figure()\n",
    "            plt.title(\"Training loss per epoch\")\n",
    "            plt.semilogy(loss, label=\"Loss\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plotly_show()       \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_envir(X_train, X_dev, X_test, model, param, verbosity, scaling):\n",
    "    \"\"\"\n",
    "    Trains and tests a NN on a given label\n",
    "    \"\"\"\n",
    "    param = 0\n",
    "    score = NN_train(X_train, model, param, scaling, verbosity = verbosity)\n",
    "    # Save model via\n",
    "    #joblib.dump(model, './data/1_neural_network.pkl')\n",
    "    # Load model via\n",
    "    #model2 = joblib.load('./data/1_neural_network.pkl')\n",
    "    predict_train = model.predict(X_train[:,3:])\n",
    "    predict_dev = model.predict(X_dev[:,3:])\n",
    "    plt.plot(X_train[:,param]-predict_train, label=(\"Train: Loss energy\"))\n",
    "    plt.plot(X_dev[:,0]-predict_dev, label=(\"Dev: Loss energy\"))\n",
    "    plt.xlabel(\"datapoint\")\n",
    "    plt.ylabel(\"Error in arb. units\")\n",
    "    plt.title(\"Error on true label\")\n",
    "    abserr_train = np.absolute(X_train[:,param]-predict_train)\n",
    "    abserr_train = np.sum(abserr_train)\n",
    "    abserr_dev = np.absolute(X_dev[:,param]-predict_dev)\n",
    "    abserr_dev = np.sum(abserr_dev)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    print(\"Mean error per prediction in training set in run %3.2f \" % (abserr_train/len(X_train)))\n",
    "    print(\"Mean error per prediction in dev set in run %3.2f \" % (abserr_dev/len(X_dev)))\n",
    "    plotly_show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin of testing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining (Hyper)Parameters\n",
    "\"\"\"\n",
    "exp = 5 # Exponent defining the size of the file\n",
    "factor = 2\n",
    "N = int(factor*10**(exp)) # Actual value\n",
    "noisee = True\n",
    "comment = \"noise_wosp\" # Comment for data file name\n",
    "data_size = int(N/1000) # Value for labeling the data (in \"kilo samples\") \n",
    "x = np.arange(2008,2018,0.01) # Grid for creating and importing data\n",
    "# Following the definition of the different \n",
    "\"\"\"\n",
    "model_energy = MLPR(hidden_layer_sizes=(100, ), activation=\"relu\", solver=\"adam\", alpha=0.0001, batch_size=\"auto\", \n",
    "             learning_rate=\"constant\", learning_rate_init=0.001, power_t=0.5, max_iter=20000, shuffle=True, \n",
    "             random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "             early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model_splitting = MLPR(hidden_layer_sizes=(100, ), activation=\"relu\", solver=\"adam\", alpha=0.0001, batch_size=\"auto\", \n",
    "             learning_rate=\"constant\", learning_rate_init=0.001, power_t=0.5, max_iter=20000, shuffle=True, \n",
    "             random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "             early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model_ratio = MLPR(hidden_layer_sizes=(100, ), activation=\"relu\", solver=\"adam\", alpha=0.0001, batch_size=\"auto\", \n",
    "             learning_rate=\"constant\", learning_rate_init=0.001, power_t=0.5, max_iter=20000, shuffle=True, \n",
    "             random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "             early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\"\"\"\n",
    "#model_energy = MLPR(hidden_layer_sizes=(100, ), max_iter=200, activation=\"relu\", verbose = True, \n",
    "#                    solver = 'adam',learning_rate = 'constant',learning_rate_init = 1e-3, batch_size = 200)\n",
    "model_energy = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_set(N, x, noise = noisee, Set = \"train\"+str(factor)+\"x\"+str(data_size)+\"k\"+comment, verbosity = 1)\n",
    "gen_set(int(N/10), x, noise = noisee, Set = \"dev\"+str(factor)+\"x\"+str(int(data_size/10))+\"k\"+comment, verbosity = 0)\n",
    "gen_set(int(N/10), x, noise = noisee, Set = \"test\"+str(factor)+\"x\"+str(int(data_size/10))+\"k\"+comment, verbosity = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = read_set(Set=\"train\"+str(factor)+\"x\"+str(data_size)+\"k\"+comment)\n",
    "X_dev = read_set(Set=\"dev\"+str(factor)+\"x\"+str(int(data_size/10))+\"k\"+comment)\n",
    "X_test = read_set(Set=\"test\"+str(factor)+\"x\"+str(int(data_size/10))+\"k\"+comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_100k = read_set(Set = \"train1x100knoise_wosp\")\n",
    "X_dev_100k = read_set(Set = \"dev1x10knoise_wosp\")\n",
    "X_test_100k = read_set(Set = \"test1x10knoise_wosp\")\n",
    "X_train_200k = read_set(Set = \"train2x200knoise_wosp\")\n",
    "X_dev_200k = read_set(Set = \"dev2x20knoise_wosp\")\n",
    "X_test_200k = read_set(Set = \"test2x20knoise_wosp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_envir(X_train_100k, X_dev_100k, X_test_100k, model_energy, param = 0, verbosity = 2, scaling = False)\n",
    "test_envir(X_train_200k, X_dev_200k, X_test_200k, model_energy, param = 0, verbosity = 2, scaling = False)\n",
    "test_envir(X_train_100k, X_dev_100k, X_test_100k, model_energy, param = 1, verbosity = 2, scaling = False)\n",
    "test_envir(X_train_200k, X_dev_200k, X_test_200k, model_energy, param = 1, verbosity = 2, scaling = False)\n",
    "test_envir(X_train_100k, X_dev_100k, X_test_100k, model_energy, param = 2, verbosity = 2, scaling = False)\n",
    "test_envir(X_train_200k, X_dev_200k, X_test_200k, model_energy, param = 2, verbosity = 2, scaling = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    plt.plot(x, X_train[i+12,3:])\n",
    "plotly_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
