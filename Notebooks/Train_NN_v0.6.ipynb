{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN v0.6: Imporved version of v0.5\n",
    "\n",
    "- generates N training samples with varied energies $K_{\\alpha_1}$, energy splitting and ratios\n",
    "- trains one MLP regressor for each parameter\n",
    "- The trained NNs are tested on training set, dev set and test set\n",
    "- No optimization have been made.\n",
    "\n",
    "Changes:\n",
    "\n",
    "- Removed:\n",
    "    - \n",
    "- Changes:\n",
    "    - gen_set(): \n",
    "        - Now norms the spectra to 1 ($\\pm$ 0.05 due to scaling and noise)\n",
    "    - read_set():\n",
    "        - No longer scales spectra\n",
    "- New:\n",
    "    - build_library():\n",
    "        - Takes number N of spectra to build and generates N spectra, divided in 1e5 subfiles. Then zips them together and deletes subfiles. Saves 80% of space, increases runtime.\n",
    "    - load_library(): \n",
    "        - Load library by unzipping library in subfiles. Reads subfiles and builds data matrix. Deletes Subfiles afterwards.\n",
    "- Bug fixes\n",
    "    - Introduced norming spectra to achieve better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import basic libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import functions and tools needed\n",
    "\"\"\"\n",
    "import os\n",
    "import plotly.offline as py\n",
    "import plotly.tools as tls\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import math\n",
    "import scipy.stats as sc\n",
    "from scipy.special import wofz\n",
    "from astropy.modeling.functional_models import Voigt1D \n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.neural_network import MLPRegressor as MLPR\n",
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "# Redirect stdout\n",
    "import io \n",
    "import sys\n",
    "# Use subpreocesses\n",
    "import subprocess\n",
    "# Use tgz to zip large files into compressed containers\n",
    "import tarfile\n",
    "# Use garbage collect to clear unused variables and free memory\n",
    "import gc\n",
    "# Control figure size\n",
    "matplotlib.rcParams['figure.figsize']=(7,5)\n",
    "py.init_notebook_mode()\n",
    "# Use plotly as gifure output\n",
    "def plotly_show():\n",
    "    fig = plt.gcf()\n",
    "    py.iplot_mpl(fig)\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_area(x, v):\n",
    "    \"\"\"\n",
    "    Calculates the area beneath a spectrum\n",
    "    \"\"\"\n",
    "    width = (x[len(x)-1]-x[0])/(len(x)-1)\n",
    "    area = 0\n",
    "    for i in range(len(x)):\n",
    "        area += v[i] * width\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_library(N, x, noise, Set, verbosity):\n",
    "    \"\"\"\n",
    "    Function wrapped around gen_set() for building the training library. \n",
    "        - If N is large, generates multiple msgpack files and then zips them.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    label_counter = 1\n",
    "    if ( N > 1e5 ):\n",
    "        # Create files with size of 1e5 until all spectra are created\n",
    "        # Check if file already exists\n",
    "        if (os.path.isfile(\"./data/\"+Set+\".tar.bz2\") == True):\n",
    "            sys.stderr.write('Error: File already exists! \\n')\n",
    "            return 0\n",
    "        tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"x:bz2\")\n",
    "        tar = tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"w:bz2\")\n",
    "        while ( N >= 1e5 ):\n",
    "            gen_set(int(1e5), x, noise, \"temp_\"+str(label_counter)+\".sublibrary\", verbosity)\n",
    "            # Zip files to one compressed container and delete single them afterwards\n",
    "            tar.add(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "            # Remove temporary file\n",
    "            os.remove(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "            N -= 1e5\n",
    "            label_counter += 1\n",
    "        tar.close()\n",
    "        end = time.time()\n",
    "        if (verbosity > 0):\n",
    "            print(\"+++++++++++++\")\n",
    "            print(\"Time for generating library: %3.2fs\" % (end-start))\n",
    "            print(\"+++++++++++++\")\n",
    "    else:\n",
    "        if (os.path.isfile(\"./data/\"+Set+\".tar.bz2\") == True):\n",
    "            sys.stderr.write('Error: File already exists! \\n')\n",
    "            return 0\n",
    "        tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"x:bz2\")\n",
    "        tar = tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"w:bz2\")\n",
    "        gen_set(int(N), x, noise, \"temp_\"+str(label_counter)+\".sublibrary\", verbosity)\n",
    "        tar.add(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "        os.remove(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "        end = time.time()\n",
    "        if (verbosity > 0):\n",
    "            print(\"+++++++++++++\")\n",
    "            print(\"Time for generating library: %3.2fs\" % (end-start))\n",
    "            print(\"+++++++++++++\")\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_set(N, x, noise, Set, verbosity): \n",
    "    \"\"\"\n",
    "    - Generates a set with N spectra by using the superposition of TWO Voigt profiles with randomly choosen\n",
    "        parameters \n",
    "            gamma1: HWHM of Lorentzian part of Voigt profile 1 \n",
    "            gamma2: HWHM of Lorentzian part of Voigt profile 2\n",
    "            sigma1: Standard uncertainty of Gaussian part of Voigt profile 1\n",
    "            sigma2: Standard uncertainty of Gaussian part of Voigt profile 2\n",
    "            epsilon: offset to energy E\n",
    "        The Energy E (K alpha1) is centered around 2014eV \n",
    "    \"\"\"\n",
    "    if (verbosity > 0):\n",
    "        start = time.time()\n",
    "    # Definition of some parameters\n",
    "    gamma1, sigma1 = 0.345, 0.07\n",
    "    gamma2, sigma2 = 0.36, 0.08\n",
    "    labels = np.array(0)\n",
    "    labels = np.delete(labels, 0)\n",
    "    # Creating the empty data matrix with dimensions N x d+1\n",
    "    X = np.zeros((N,len(x)+3))\n",
    "    runtime = np.array(0)\n",
    "    runtime = np.delete(runtime, 0)\n",
    "    \"\"\"\n",
    "    For loop loops N times to create N spectra. The single spectrum is evaluate and fitted\n",
    "    on range x to get equal x values as features (Note: When trained on grid defined by x then\n",
    "    real data must also be sampled on same grid!). File format:\n",
    "        File dimensions: N x (2 + d), where d is number of grid points resulting from grid x\n",
    "        [E dE x1 x2 ... xd]\n",
    "    \"\"\"\n",
    "    if (verbosity > 0):\n",
    "        loop_time_start = time.time()\n",
    "    for i in range(N):\n",
    "        # Generate random distribution (+- 1) around central value of energie E\n",
    "        E_epsilon = (np.random.random_sample()-0.5)*2\n",
    "        # Generate random distribution (+- 0.1) around central value of energie dE\n",
    "        dE_epsilon = (np.random.random_sample()-0.5)*(0.4)\n",
    "        # Generate random distribution (+- 0.05) around central value of amplitude L1\n",
    "        dL1 = (np.random.random_sample()-0.5)/15\n",
    "        # Generate random distribution (+- 0.05) around central value of amplitude L2\n",
    "        dL2 = (np.random.random_sample()-0.5)/15\n",
    "        L1 = 0.18 + dL1\n",
    "        L2 = 0.3 + dL2\n",
    "        E = 2014 + E_epsilon\n",
    "        dE = 0.85 + dE_epsilon\n",
    "        v1 = Voigt1D(x_0=E-dE, amplitude_L=L1, fwhm_L=2*gamma1, fwhm_G=2*sigma1*np.sqrt(2*np.log(2)))\n",
    "        v2 = Voigt1D(x_0=E, amplitude_L=L2, fwhm_L=2*gamma2, fwhm_G=2*sigma2*np.sqrt(2*np.log(2)))\n",
    "        # Calculate the ratio of the areas\n",
    "        R = calc_area(x, v2(x))/calc_area(x,v1(x))\n",
    "        # Save values in two columns in form of [x,v(x)]\n",
    "        X[i,0] = E\n",
    "        X[i,1] = dE\n",
    "        X[i,2] = R\n",
    "        if (noise == True):\n",
    "            # Superpose Voigt profiles and norm them to 1\n",
    "            append = v1(x)+v2(x)\n",
    "            # Normalize spectrum to 1\n",
    "            amp = np.amax(append)\n",
    "            append  /= amp\n",
    "            # Apply poisson noise to the data, magnify amplitudes to get poisson function working\n",
    "            append =np.multiply(append,0.5*1e4)\n",
    "            append = np.random.poisson(append)\n",
    "            # Scale down again\n",
    "            append = np.divide(append, 0.5*1e4)\n",
    "            # Fill data matrix\n",
    "            for j in range(len(x)):\n",
    "                X[i,j+3] = append[j]\n",
    "        else: \n",
    "            # Superpose Voigt profiles and norm them to 1\n",
    "            append = v1(x)+v2(x)\n",
    "            # Normalize spectrum to 1\n",
    "            amp = np.amax(append)\n",
    "            append  /= amp\n",
    "            for j in range(len(x)):\n",
    "                X[i,j+3] = append[j]\n",
    "        # Runtime control\n",
    "        if (verbosity > 0):\n",
    "            if ( i % (N/10) == 0 ):\n",
    "                loop_time_end = time.time()\n",
    "                time_diff = loop_time_end-loop_time_start\n",
    "                runtime = np.append(runtime, time_diff)\n",
    "                print(\"Progress: %i/%i, time for loop: %3.2fs\" % (i , N, time_diff))\n",
    "                loop_time_start = time.time()\n",
    "    # Create pandas dataframe\n",
    "    df_spectrum = pd.DataFrame(X)\n",
    "    df_spectrum.to_msgpack('./data/'+Set+'.spectrum')\n",
    "    # Plot verbosity information about the loop\n",
    "    if (verbosity > 0):\n",
    "        end = time.time()\n",
    "        print(\"Time for generating the \"+Set+\" set:\", end-start)\n",
    "        if (verbosity > 1):\n",
    "            plt.figure()\n",
    "            plt.title(\"Runtime for generating the data set \"+Set)\n",
    "            plt.plot(runtime, label=\"Runtime per N/10 loops\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plotly_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_library(Set, fraction, verbosity):\n",
    "    \"\"\"\n",
    "    Function wrapped around read_set() for loading the training library. \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    tar = tarfile.open(\"./data/\"+Set+\".tar.bz2\", \"r:bz2\")\n",
    "    label_counter = 1\n",
    "    X = np.array(0)\n",
    "    X = np.delete(X, 0)\n",
    "    for i in (tar):\n",
    "        if ( label_counter > fraction ):\n",
    "            break\n",
    "        start_loop = time.time()\n",
    "        tar.extract(i)\n",
    "        temp = read_set(\"temp_\"+str(label_counter)+\".sublibrary\")\n",
    "        N = len(temp) * label_counter\n",
    "        d = len(temp[0])\n",
    "        X = np.append(X, temp).reshape(N, d)\n",
    "        end_loop = time.time()\n",
    "        if (verbosity > 0):\n",
    "            print(\"Time for unpacking: %3.2fs\" % (end_loop-start_loop))\n",
    "        os.remove(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "        label_counter += 1\n",
    "    tar.close()\n",
    "    end = time.time()\n",
    "    if (verbosity > 0):\n",
    "        print(\"+++++++++++\")\n",
    "        print(\"Time for loading the library: %3.2fs\" % (end-start))\n",
    "        print(\"+++++++++++\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_set(Set):\n",
    "    \"\"\" \n",
    "    Read data and store it in (Nxd) Martix, where N donates \n",
    "    the observation (single spectrum) and d the dth feature \n",
    "    (datapoint given by choosing x). The data gets fitted \n",
    "    by the Splines fit. Also, noise is added when reading the\n",
    "    data if flag is set.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    X = pd.read_msgpack('./data/'+Set+'.spectrum') \n",
    "    X = X.as_matrix()\n",
    "    end = time.time()\n",
    "    print(\"Time for reading \"+Set+\" set: %3.2fs\" % (end-start))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_input(X):\n",
    "    \"\"\"\n",
    "    Feature skaling for NN apporach. It is \"highly recommended\" to scale input data to either [0:1] or [-1:+1] \n",
    "    or standardize it to have mean 0 and variance 1\n",
    "    Source:\n",
    "    http://scikit-learn.org/stable/modules/neural_networks_supervised.html#regression\n",
    "    This function standardizes X \n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler  \n",
    "    scaler = StandardScaler()  \n",
    "    # Don't cheat - fit only on training data\n",
    "    N = len(X)\n",
    "    d = len(X[0]+3)\n",
    "    y = X[:,:3]\n",
    "    X = X[:,3:]\n",
    "    scaler.fit(X)  \n",
    "    X = scaler.transform(X) \n",
    "    X = np.append(y,X, axis=1)\n",
    "    #for i in range(len(X)):\n",
    "    #    plt.plot(x,X[i])\n",
    "    #plt.grid(True)\n",
    "    #plotly_show()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_train(X, model, parameter, scaling, verbosity):\n",
    "    \"\"\"\n",
    "    Trains given model on data X and labels y. Returns trainings score\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    y = np.ravel(X[:,parameter])\n",
    "    if (scaling == True):\n",
    "        X = scale_input(X)\n",
    "    X = X[:,3:]\n",
    "    # Set out pipe to catch stdout for getting verbosity output of model.fit\n",
    "    if (verbosity > 0):\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = mystdout = io.StringIO()\n",
    "    model.fit(X, y)\n",
    "    # Delete pipe\n",
    "    if (verbosity > 0):\n",
    "        sys.stdout = old_stdout\n",
    "    # Save verbosity output (training loss) in variable\n",
    "    loss = np.array(0)\n",
    "    loss = np.delete(loss, 0)\n",
    "    if (verbosity > 0):\n",
    "        verbosity_output = mystdout.getvalue()\n",
    "        verbosity_output = np.array(verbosity_output.split(' '))\n",
    "        for i in range(4,len(verbosity_output),4):\n",
    "            if (verbosity_output[i].split('\\n')[0] == 'improve'):\n",
    "                break\n",
    "            else:\n",
    "                loss = np.append(loss, float(verbosity_output[i].split('\\n')[0]))\n",
    "    # Save score of training\n",
    "    score = model.score(X,y)\n",
    "    end = time.time()\n",
    "    # Print training statistics depending onb verbosity level\n",
    "    if (verbosity > 0):\n",
    "        print(\"Training time: %3.2f \" % (end - start))\n",
    "        print(\"Training score: %3.2f \" % (score))\n",
    "        if (verbosity > 1):\n",
    "            plt.figure()\n",
    "            plt.title(\"Training loss per epoch\")\n",
    "            plt.semilogy(loss, label=\"Loss\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plotly_show()       \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_envir(X_train, X_dev, X_test, model, param, verbosity, scaling):\n",
    "    \"\"\"\n",
    "    Trains and tests a NN on a given label\n",
    "    \"\"\"\n",
    "    param = 0\n",
    "    score = NN_train(X_train, model, param, scaling, verbosity = verbosity)\n",
    "    #Save model via\n",
    "    joblib.dump(model, './data/1_neural_network.pkl')\n",
    "    #Load model via\n",
    "    model2 = joblib.load('./data/1_neural_network.pkl')\n",
    "    predict_train = model.predict(X_train[:,3:])\n",
    "    predict_dev = model.predict(X_dev[:,3:])\n",
    "    plt.plot(X_train[:,param]-predict_train, label=(\"Train: Loss energy\"))\n",
    "    plt.plot(X_dev[:,0]-predict_dev, label=(\"Dev: Loss energy\"))\n",
    "    plt.xlabel(\"datapoint\")\n",
    "    plt.ylabel(\"Error in arb. units\")\n",
    "    plt.title(\"Error on true label\")\n",
    "    abserr_train = np.absolute(X_train[:,param]-predict_train)\n",
    "    abserr_train = np.sum(abserr_train)\n",
    "    abserr_dev = np.absolute(X_dev[:,param]-predict_dev)\n",
    "    abserr_dev = np.sum(abserr_dev)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    print(\"Mean error per prediction in training set in run %3.2f \" % (abserr_train/len(X_train)))\n",
    "    print(\"Mean error per prediction in dev set in run %3.2f \" % (abserr_dev/len(X_dev)))\n",
    "    plotly_show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin of testing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining (Hyper)Parameters\n",
    "\"\"\"\n",
    "exp = 5 # Exponent defining the size of the file\n",
    "factor = 1\n",
    "N = int(factor*10**(exp)) # Actual value\n",
    "noisee = True\n",
    "comment = \"less_features\" # Comment for data file name\n",
    "data_size = str(int(N/1000))+\"k_\" # Value for labeling the data (in \"kilo samples\") \n",
    "x = np.arange(2008,2018,0.1) # Grid for creating and importing data\n",
    "# Following the definition of the different \n",
    "model_energy = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "model_splitting = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "model_ratios = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build_library(N,x,noise=True,Set=\"train\",verbosity=1)\n",
    "#build_library(int(N/10),x,noise=True,Set=\"test\",verbosity=1)\n",
    "#build_library(int(N/10),x,noise=True,Set=\"dev\",verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = load_library(Set=\"train\", fraction=1000, verbosity=0)\n",
    "Xdev = load_library(Set=\"dev\", fraction=1000, verbosity=0)\n",
    "Xtest = load_library(Set=\"test\", fraction=1000, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_envir(Xtrain, Xdev, Xtest, model_energy, param = 0, verbosity=2, scaling=False)\n",
    "test_envir(Xtrain, Xdev, Xtest, model_splitting, param = 1, verbosity=2, scaling=False)\n",
    "test_envir(Xtrain, Xdev, Xtest, model_ratios, param = 2, verbosity=2, scaling=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
