{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN v0.7: Adding Classifier (WORK IN PROGRESS)\n",
    "\n",
    "- generates N training samples with varied energies $K_{\\alpha_1}$, energy splitting and ratios\n",
    "- trains one MLP regressor for each parameter\n",
    "- Also implements possibility to use classification on fixed energy grid\n",
    "- The trained NNs are tested on training set, dev set and test set\n",
    "- No optimization have been made.\n",
    "\n",
    "Changes:\n",
    "\n",
    "- Removed:\n",
    "\n",
    "- Changes:\n",
    "    - Changed some function parameters: Now X and Y are treated seperately \n",
    "- New:\n",
    "    - global variable ob total number labels saved in a library\n",
    "    - split_library(library): splits library into training, dev, and test set with 80%,10%,10% ratio\n",
    "- Bug fixes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import basic libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import functions and tools needed\n",
    "\"\"\"\n",
    "import os\n",
    "import plotly.offline as py\n",
    "import plotly.tools as tls\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import math\n",
    "import scipy.stats as sc\n",
    "from scipy import interpolate\n",
    "from scipy.special import wofz\n",
    "from astropy.modeling.functional_models import Voigt1D \n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.neural_network import MLPRegressor as MLPR\n",
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "import sklearn as skl\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "# Redirect stdout\n",
    "import io \n",
    "import sys\n",
    "# Use subpreocesses\n",
    "import subprocess\n",
    "# Use tgz to zip large files into compressed containers\n",
    "import tarfile\n",
    "# Use garbage collect to clear unused variables and free memory\n",
    "import gc\n",
    "# Control figure size\n",
    "matplotlib.rcParams['figure.figsize']=(7,5)\n",
    "py.init_notebook_mode()\n",
    "# Use plotly as gifure output\n",
    "def plotly_show():\n",
    "    fig = plt.gcf()\n",
    "    plotlyfig = tls.mpl_to_plotly(fig,resize=True)\n",
    "    plotlyfig['layout']['showlegend']=True\n",
    "    py.iplot(plotlyfig)\n",
    "# define global variables\n",
    "n_labels = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_area(x, v):\n",
    "    \"\"\"\n",
    "    Calculates the area beneath a spectrum\n",
    "    \"\"\"\n",
    "    width = (x[len(x)-1]-x[0])/(len(x)-1)\n",
    "    area = 0\n",
    "    for i in range(len(x)):\n",
    "        area += v[i] * width\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_library(N, x, noise, Set, verbosity):\n",
    "    \"\"\"\n",
    "    Function wrapped around gen_set() for building the training library. \n",
    "        - If N is large, generates multiple msgpack files and then zips them.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    label_counter = 1\n",
    "    if ( N > 1e5 ):\n",
    "        # Create files with size of 1e5 until all spectra are created\n",
    "        # Check if file already exists\n",
    "        if (os.path.isfile(\"./data/\"+Set+\".tar.bz2\") == True):\n",
    "            sys.stderr.write('Error: File already exists! \\n')\n",
    "            return 0\n",
    "        tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"x:bz2\")\n",
    "        tar = tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"w:bz2\")\n",
    "        while ( N >= 1e5 ):\n",
    "            gen_set(int(1e5), x, noise, \"temp_\"+str(label_counter)+\".sublibrary\", verbosity)\n",
    "            # Zip files to one compressed container and delete single them afterwards\n",
    "            tar.add(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "            # Remove temporary file\n",
    "            os.remove(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "            N -= 1e5\n",
    "            label_counter += 1\n",
    "        tar.close()\n",
    "        end = time.time()\n",
    "        if (verbosity > 0):\n",
    "            print(\"+++++++++++++\")\n",
    "            print(\"Time for generating library: %3.2fs\" % (end-start))\n",
    "            print(\"+++++++++++++\")\n",
    "    else:\n",
    "        if (os.path.isfile(\"./data/\"+Set+\".tar.bz2\") == True):\n",
    "            sys.stderr.write('Error: File already exists! \\n')\n",
    "            return 0\n",
    "        tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"x:bz2\")\n",
    "        tar = tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"w:bz2\")\n",
    "        gen_set(int(N), x, noise, \"temp_\"+str(label_counter)+\".sublibrary\", verbosity)\n",
    "        tar.add(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "        os.remove(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "        end = time.time()\n",
    "        if (verbosity > 0):\n",
    "            print(\"+++++++++++++\")\n",
    "            print(\"Time for generating library: %3.2fs\" % (end-start))\n",
    "            print(\"+++++++++++++\")\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_set(N, x, noise, Set, verbosity): \n",
    "    \"\"\"\n",
    "    - Generates a set with N spectra by using the superposition of TWO Voigt profiles with randomly choosen\n",
    "        parameters \n",
    "            gamma1: HWHM of Lorentzian part of Voigt profile 1 \n",
    "            gamma2: HWHM of Lorentzian part of Voigt profile 2\n",
    "            sigma1: Standard uncertainty of Gaussian part of Voigt profile 1\n",
    "            sigma2: Standard uncertainty of Gaussian part of Voigt profile 2\n",
    "            epsilons: offset to energy E, dE, Ratios\n",
    "        The Energy E (K alpha1) is centered around 2014eV \n",
    "        Splitting is set to 0.85eV +- 0.05\n",
    "        Ratios are set to 1.7 pm 0.5\n",
    "    \"\"\"\n",
    "    if (verbosity > 0):\n",
    "        start = time.time()\n",
    "    # Definition of some parameters\n",
    "    gamma1, sigma1 = 0.345, 0.07\n",
    "    gamma2, sigma2 = 0.36, 0.08\n",
    "    labels = np.array(0)\n",
    "    labels = np.delete(labels, 0)\n",
    "    # Creating the empty data matrix with dimensions N x d+4 (+4 due to label)\n",
    "    X = np.zeros((N,len(x)+n_labels))\n",
    "    runtime = np.array(0)\n",
    "    runtime = np.delete(runtime, 0)\n",
    "    \"\"\"\n",
    "    For loop loops N times to create N spectra. The single spectrum is evaluate and fitted\n",
    "    on range x to get equal x values as features (Note: When trained on grid defined by x then\n",
    "    real data must also be sampled on same grid!). File format:\n",
    "        File dimensions: N x (2 + d), where d is number of grid points resulting from grid x\n",
    "        [E dE x1 x2 ... xd]\n",
    "    \"\"\"\n",
    "    if (verbosity > 0):\n",
    "        loop_time_start = time.time()\n",
    "    for i in range(N):\n",
    "        # Generate random distribution (+- 1) around central value of energie E\n",
    "        E_epsilon = (np.random.random_sample()-0.5)*2\n",
    "        # Generate random distribution (+- 0.1) around central value of energie dE\n",
    "        dE_epsilon = (np.random.random_sample()-0.5)*(0.4)\n",
    "        # Generate random distribution (+- 0.05) around central value of amplitude L1\n",
    "        dL1 = (np.random.random_sample()-0.5)/15\n",
    "        # Generate random distribution (+- 0.05) around central value of amplitude L2\n",
    "        dL2 = (np.random.random_sample()-0.5)/15\n",
    "        L1 = 0.18 + dL1\n",
    "        L2 = 0.3 + dL2\n",
    "        E = 2014 + E_epsilon\n",
    "        dE = 0.85 + dE_epsilon\n",
    "        v1 = Voigt1D(x_0=E-dE, amplitude_L=L1, fwhm_L=2*gamma1, fwhm_G=2*sigma1*np.sqrt(2*np.log(2)))\n",
    "        v2 = Voigt1D(x_0=E, amplitude_L=L2, fwhm_L=2*gamma2, fwhm_G=2*sigma2*np.sqrt(2*np.log(2)))\n",
    "        # Calculate the ratio of the areas\n",
    "        R = calc_area(x, v2(x))/calc_area(x,v1(x))\n",
    "        ### Discretize Energies for classification option\n",
    "        # Define energy bins\n",
    "        bins = np.arange(2013,2015+0.01,0.01)\n",
    "        # Apply this grid on enegery and center energy to the bin center to increase precision\n",
    "        binned = np.digitize(E, bins, right=True)\n",
    "        E_binned = bins[binned]#-0.005\n",
    "        # Save values in two columns in form of [x,v(x)]\n",
    "        X[i,0] = E\n",
    "        X[i,1] = E_binned\n",
    "        X[i,2] = dE\n",
    "        X[i,3] = R\n",
    "        if (noise == True):\n",
    "            # Superpose Voigt profiles and norm them to 1\n",
    "            append = v1(x)+v2(x)\n",
    "            # Normalize spectrum to 1\n",
    "            amp = np.amax(append)\n",
    "            append  /= amp\n",
    "            # Apply poisson noise to the data, magnify amplitudes to get poisson function working\n",
    "            append =np.multiply(append,0.5*1e4)\n",
    "            append = np.random.poisson(append)\n",
    "            # Scale down again\n",
    "            append = np.divide(append, 0.5*1e4)\n",
    "            # Fill data matrix\n",
    "            for j in range(len(x)):\n",
    "                X[i,j+n_labels] = append[j]\n",
    "        else: \n",
    "            # Superpose Voigt profiles and norm them to 1\n",
    "            append = v1(x)+v2(x)\n",
    "            # Normalize spectrum to 1\n",
    "            amp = np.amax(append)\n",
    "            append  /= amp\n",
    "            for j in range(len(x)):\n",
    "                X[i,j+n_labels] = append[j]\n",
    "        # Runtime control\n",
    "        if (verbosity > 0):\n",
    "            if ( i % (N/10) == 0 ):\n",
    "                loop_time_end = time.time()\n",
    "                time_diff = loop_time_end-loop_time_start\n",
    "                runtime = np.append(runtime, time_diff)\n",
    "                print(\"Progress: %i/%i, time for loop: %3.2fs\" % (i , N, time_diff))\n",
    "                loop_time_start = time.time()\n",
    "    # Create pandas dataframe\n",
    "    df_spectrum = pd.DataFrame(X)\n",
    "    df_spectrum.to_msgpack('./data/'+Set+'.spectrum')\n",
    "    # Plot verbosity information about the loop\n",
    "    if (verbosity > 0):\n",
    "        end = time.time()\n",
    "        print(\"Time for generating the \"+Set+\" set:\", end-start)\n",
    "        if (verbosity > 1):\n",
    "            plt.figure()\n",
    "            plt.title(\"Runtime for generating the data set \"+Set)\n",
    "            plt.plot(runtime, label=\"Runtime per N/10 loops\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plotly_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_library(Set, fraction, verbosity):\n",
    "    \"\"\"\n",
    "    Function wrapped around read_set() for loading the training library. \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    tar = tarfile.open(\"./data/\"+Set+\".tar.bz2\", \"r:bz2\")\n",
    "    label_counter = 1\n",
    "    X = np.array(0)\n",
    "    X = np.delete(X, 0)\n",
    "    for i in (tar):\n",
    "        if ( label_counter > fraction ):\n",
    "            break\n",
    "        start_loop = time.time()\n",
    "        tar.extract(i)\n",
    "        temp = read_set(\"temp_\"+str(label_counter)+\".sublibrary\")\n",
    "        N = len(temp) * label_counter\n",
    "        d = len(temp[0])\n",
    "        X = np.append(X, temp).reshape(N, d)\n",
    "        end_loop = time.time()\n",
    "        if (verbosity > 0):\n",
    "            print(\"Time for unpacking: %3.2fs\" % (end_loop-start_loop))\n",
    "        os.remove(\"./data/temp_\"+str(label_counter)+\".sublibrary.spectrum\")\n",
    "        label_counter += 1\n",
    "    tar.close()\n",
    "    end = time.time()\n",
    "    if (verbosity > 0):\n",
    "        print(\"+++++++++++\")\n",
    "        print(\"Time for loading the library: %3.2fs\" % (end-start))\n",
    "        print(\"+++++++++++\")\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_set(Set):\n",
    "    \"\"\" \n",
    "    Read data and store it in (Nxd) Martix, where N donates \n",
    "    the observation (single spectrum) and d the dth feature \n",
    "    (datapoint given by choosing x). The data gets fitted \n",
    "    by the Splines fit. Also, noise is added when reading the\n",
    "    data if flag is set.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    \n",
    "    X = pd.read_msgpack('./data/'+Set+'.spectrum') \n",
    "    X = X.as_matrix()\n",
    "    end = time.time()\n",
    "    print(\"Time for reading \"+Set+\" set: %3.2fs\" % (end-start))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_input(X):\n",
    "    \"\"\"\n",
    "    Feature skaling for NN apporach. It is \"highly recommended\" to scale input data to either [0:1] or [-1:+1] \n",
    "    or standardize it to have mean 0 and variance 1\n",
    "    Source:\n",
    "    http://scikit-learn.org/stable/modules/neural_networks_supervised.html#regression\n",
    "    This function standardizes X \n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler  \n",
    "    scaler = StandardScaler()  \n",
    "    # Don't cheat - fit only on training data\n",
    "    N = len(X)\n",
    "    d = len(X[0]+n_labels)\n",
    "    scaler.fit(X)  \n",
    "    X = scaler.transform(X) \n",
    "    #for i in range(len(X)):\n",
    "    #    plt.plot(x,X[i])\n",
    "    #plt.grid(True)\n",
    "    #plotly_show()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_train(library, model, parameter, scaling, verbosity):\n",
    "    \"\"\"\n",
    "    Trains given model on data X and labels y. Returns trainings score\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    Xtrain, Ytrain, Xdev, Ydev, Xtest, Ytest = split_library(library)\n",
    "    X = Xtrain\n",
    "    y = np.ravel(Ytrain[:,parameter])*1e3 # multiply to create an integer problem\n",
    "    y = y.astype(int)\n",
    "    if (scaling == True):\n",
    "        X = scale_input(X)\n",
    "    # Set out pipe to catch stdout for getting verbosity output of model.fit\n",
    "    if (verbosity > 0):\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = mystdout = io.StringIO()\n",
    "    model.fit(X, y)\n",
    "    # Delete pipe\n",
    "    if (verbosity > 0):\n",
    "        sys.stdout = old_stdout\n",
    "    # Save verbosity output (training loss) in variable\n",
    "    loss = np.array(0)\n",
    "    loss = np.delete(loss, 0)\n",
    "    if (verbosity > 0):\n",
    "        verbosity_output = mystdout.getvalue()\n",
    "        verbosity_output = np.array(verbosity_output.split(' '))\n",
    "        for i in range(4,len(verbosity_output),4):\n",
    "            if (verbosity_output[i].split('\\n')[0] == 'improve'):\n",
    "                break\n",
    "            else:\n",
    "                loss = np.append(loss, float(verbosity_output[i].split('\\n')[0]))\n",
    "    # Save score of training\n",
    "    score = model.score(X,y)\n",
    "    end = time.time()\n",
    "    # Print training statistics depending onb verbosity level\n",
    "    if (verbosity > 0):\n",
    "        print(\"Training time: %3.2f \" % (end - start))\n",
    "        print(\"Training score: %3.2f \" % (score))\n",
    "        if (verbosity > 1):\n",
    "            plt.figure()\n",
    "            plt.title(\"Training loss per epoch\")\n",
    "            plt.semilogy(loss, label=\"Loss\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plotly_show()       \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_envir(X_train, Y_train, X_dev, Y_dev, X_test, Y_test, model, param, verbosity, scaling):\n",
    "    \"\"\"\n",
    "    Trains and tests a NN on a given label\n",
    "    \"\"\"\n",
    "    param = 0\n",
    "    score = NN_train(X_train, Y_train, model, param, scaling, verbosity = verbosity)\n",
    "    #Save model via\n",
    "    joblib.dump(model, './data/1_neural_network.pkl')\n",
    "    #Load model via\n",
    "    model2 = joblib.load('./data/1_neural_network.pkl')\n",
    "    predict_train = model.predict(X_train)\n",
    "    predict_dev = model.predict(X_dev)\n",
    "    plt.plot(Y_train[:,param]-predict_train, label=(\"Train: Loss energy\"))\n",
    "    plt.plot(Y_dev[:,param]-predict_dev, label=(\"Dev: Loss energy\"))\n",
    "    plt.xlabel(\"datapoint\")\n",
    "    plt.ylabel(\"Error in arb. units\")\n",
    "    plt.title(\"Error on true label\")\n",
    "    abserr_train = np.absolute(Y_train[:,param]-predict_train)\n",
    "    abserr_train = np.sum(abserr_train)\n",
    "    abserr_dev = np.absolute(Y_dev[:,param]-predict_dev)\n",
    "    abserr_dev = np.sum(abserr_dev)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    print(\"Mean error per prediction in training set in run %3.2f \" % (abserr_train/len(X_train)))\n",
    "    print(\"Mean error per prediction in dev set in run %3.2f \" % (abserr_dev/len(X_dev)))\n",
    "    plotly_show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_library(library):\n",
    "    \"\"\"\n",
    "    Function to split a library in training, dev, and test set after good 'ol ratio 80%:10%:10%\n",
    "    \"\"\"\n",
    "    N = len(library)\n",
    "    N_train = int(N*0.8)\n",
    "    N_dev = N_test = int(N*0.1)\n",
    "    d_features = len(library[0,n_labels:])\n",
    "    d_labels = n_labels\n",
    "    Xtrain = library[0:N_train,n_labels:]\n",
    "    Ytrain = library[0:N_train,0:n_labels]\n",
    "    Xdev = library[N_train:N_train+N_dev,n_labels:]\n",
    "    Ydev = library[N_train:N_train+N_dev,0:n_labels]\n",
    "    Xtest = library[N_train+N_dev:N_train+N_dev+N_test,n_labels:]\n",
    "    Ytest = library[N_train+N_dev:N_train+N_dev+N_test,0:n_labels]\n",
    "    return Xtrain, Ytrain, Xdev, Ydev, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin of testing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining (Hyper)Parameters\n",
    "\"\"\"\n",
    "exp = 5 # Exponent defining the size of the file\n",
    "factor = 1\n",
    "N = int(factor*10**(exp)) # Actual value\n",
    "noisee = True\n",
    "comment = \"less_features_binned_energy_not_centered\" # Comment for data file name\n",
    "data_size = str(int(N/1000))+\"k_\" # Value for labeling the data (in \"kilo samples\") \n",
    "set_name = data_size+comment+\"_library\"\n",
    "x = np.arange(2008,2018,0.1) # Grid for creating and importing data\n",
    "# Following the definition of the different \n",
    "modelR_energy = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelR_splitting = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelR_ratios = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelC_energy = MLPC(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelC_splitting = MLPC(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelC_ratios = MLPC(max_iter=2000,  activation=\"relu\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_library(N,x,noise=True,Set=set_name,verbosity=False)\n",
    "library = load_library(Set = set_name, fraction = 1000, verbosity=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_train(library, model=modelC_energy, parameter=1, scaling=False,verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xset = library[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = modelC_energy.predict(Xset)/1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ytrain[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(2013,2015,0.01)\n",
    "data = modelC_energy.predict_proba(Xset)[1]\n",
    "plt.bar(x, data)\n",
    "plotly_show()\n",
    "print(library[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Ytrain, Xdev, Ydev, Xtest, Ytest = split_library(library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_test = modelC_energy.predict(Xtest)/1e3\n",
    "predict_test_proba = modelC_energy.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(predict_test_proba[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x, predict_test_proba[0])\n",
    "plotly_show()\n",
    "print(Ytest[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Example_PKalpha_Data/GaP_Kalpha_1.30.18export.csv\")\n",
    "data = np.asmatrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(2008,2018,0.1)\n",
    "sp = interpolate.InterpolatedUnivariateSpline(data[:,0], data[:,1])\n",
    "plt.plot(data[:,0], data[:,1]/np.amax(data[:,1]), label=\"raw\")\n",
    "plt.plot(x, sp(x)/np.amax(sp(x)), label=\"interpol\")\n",
    "plt.legend()\n",
    "plotly_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(2008,2018,0.1)\n",
    "predict_data = modelC_energy.predict(sp(x).reshape(1,-1))/1e3\n",
    "predict_data_proba = modelC_energy.predict_proba(sp(x).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(np.ravel(predict_data_proba)), np.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_data)\n",
    "print(predict_data_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(2009,2019,0.1)\n",
    "print(predict_data)\n",
    "plt.gca()\n",
    "plt.xlim(2009,2019)\n",
    "plt.ylim(0,1.1)\n",
    "plt.plot(data[:,0], data[:,1]/np.amax(data[:,1]), label=\"raw\")\n",
    "plt.plot(x, sp(x)/np.amax(sp(x)), label=\"interpol\")\n",
    "plt.bar(np.arange(2013,2015,0.01), np.ravel(predict_data_proba))\n",
    "plt.legend()\n",
    "plotly_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./Example_PKalpha_Data/3.1.18.soildata_bgsubtracted.csv\", index_col = 0, header = None).T\n",
    "data = np.asmatrix(df)\n",
    "x = np.arange(2008,2018,0.1)\n",
    "sp = interpolate.UnivariateSpline(data[:,0], data[:,1], s=1e4, k=4)\n",
    "plt.plot(data[:,0], data[:,1]/np.amax(data[:,1]), label=\"raw\")\n",
    "plt.plot(x, sp(x)/np.amax(sp(x)), label=\"interpol\")\n",
    "plt.legend()\n",
    "plotly_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(2008,2018,0.1)\n",
    "predict_data = modelC_energy.predict(sp(x).reshape(1,-1))/1e3\n",
    "predict_data_proba = modelC_energy.predict_proba(sp(x).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(2008,2018,0.1)\n",
    "print(predict_data)\n",
    "plt.gca()\n",
    "plt.xlim(2009,2018)\n",
    "plt.ylim(0,1.1)\n",
    "plt.plot(data[:,0], data[:,1]/np.amax(data[:,1]), label=\"raw\")\n",
    "plt.plot(x, sp(x)/np.amax(sp(x)), label=\"interpol\")\n",
    "plt.bar(np.arange(2013,2015,0.01), np.ravel(predict_data_proba))\n",
    "plt.legend()\n",
    "plotly_show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_train(library, model=modelC_energy, parameter=1, scaling=False,verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
