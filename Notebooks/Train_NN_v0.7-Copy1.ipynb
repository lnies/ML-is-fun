{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NN v0.7: Adding Classifier (WORK IN PROGRESS)\n",
    "\n",
    "- generates N training samples with varied energies $K_{\\alpha_1}$, energy splitting and ratios\n",
    "- trains one MLP regressor for each parameter\n",
    "- Also implements possibility to use classification on fixed energy grid\n",
    "- The trained NNs are tested on training set, dev set and test set\n",
    "- No optimization have been made.\n",
    "\n",
    "Changes:\n",
    "\n",
    "- Removed:\n",
    "\n",
    "- Changes:\n",
    "    - Changed some function parameters: Now X and Y are treated seperately \n",
    "- New:\n",
    "    - global variable ob total number labels saved in a library\n",
    "    - split_library(library): splits library into training, dev, and test set with 80%,10%,10% ratio\n",
    "- Bug fixes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import basic libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import functions and tools needed\n",
    "\"\"\"\n",
    "import os\n",
    "import plotly.offline as py\n",
    "import plotly.tools as tls\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import math\n",
    "import scipy.stats as sc\n",
    "from scipy import interpolate\n",
    "from scipy.special import wofz\n",
    "from astropy.modeling.functional_models import Voigt1D \n",
    "from scipy.interpolate import UnivariateSpline\n",
    "# several sklearn functions\n",
    "from sklearn.neural_network import MLPRegressor as MLPR\n",
    "from sklearn.neural_network import MLPClassifier as MLPC\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import sklearn as skl\n",
    "import time\n",
    "from sklearn.externals import joblib\n",
    "# Redirect stdout\n",
    "import io \n",
    "import sys\n",
    "# Use subpreocesses\n",
    "import subprocess\n",
    "# Use tgz to zip large files into compressed containers\n",
    "import tarfile\n",
    "# Use garbage collect to clear unused variables and free memory\n",
    "import gc\n",
    "# Export data as different file formats\n",
    "import msgpack\n",
    "import h5py\n",
    "import dill # Saves everything! \n",
    "# Extra functions for dictionaries\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "# Control figure size\n",
    "matplotlib.rcParams['figure.figsize']=(7,5)\n",
    "py.init_notebook_mode()\n",
    "# Use plotly as gifure output\n",
    "def plotly_show():\n",
    "    fig = plt.gcf()\n",
    "    plotlyfig = tls.mpl_to_plotly(fig,resize=True)\n",
    "    plotlyfig['layout']['showlegend']=True\n",
    "    py.iplot(plotlyfig)\n",
    "# define global variables\n",
    "legend = ['states', 'energies', 'energies_binned', 'splittings', 'ratios', 'features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_area(x, v):\n",
    "    \"\"\"\n",
    "    Calculates the area beneath a spectrum\n",
    "    \"\"\"\n",
    "    width = (x[len(x)-1]-x[0])/(len(x)-1)\n",
    "    area = 0\n",
    "    for i in range(len(x)):\n",
    "        area += v[i] * width\n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_library(N, n, x, noise, Set, verbosity):\n",
    "    \"\"\"\n",
    "    Function wrapped around gen_set() for building the training library. \n",
    "        - If N is large, generates multiple msgpack files and then zips them.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    label_counter = 1\n",
    "    if ( N > 1e5 ):\n",
    "        # Create files with size of 1e5 until all spectra are created\n",
    "        # Check if file already exists\n",
    "        if (os.path.isfile(\"./data/\"+Set+\".tar.bz2\") == True):\n",
    "            sys.stderr.write('Error: File already exists! \\n')\n",
    "            return 0\n",
    "        tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"x:bz2\")\n",
    "        tar = tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"w:bz2\")\n",
    "        while ( N >= 1e5 ):\n",
    "            gen_set(int(1e5), n, x, noise, \"temp_\"+str(label_counter)+\".sublibrary\", verbosity)\n",
    "            # Zip files to one compressed container and delete single them afterwards\n",
    "            tar.add(\"./data/temp_\"+str(label_counter)+\".sublibrary.\")\n",
    "            # Remove temporary file\n",
    "            os.remove(\"./data/temp_\"+str(label_counter)+\".sublibrary.\")\n",
    "            N -= 1e5\n",
    "            label_counter += 1\n",
    "        tar.close()\n",
    "        end = time.time()\n",
    "        if (verbosity > 0):\n",
    "            print(\"+++++++++++++\")\n",
    "            print(\"Time for generating library: %3.2fs\" % (end-start))\n",
    "            print(\"+++++++++++++\")\n",
    "    else:\n",
    "        if (os.path.isfile(\"./data/\"+Set+\".tar.bz2\") == True):\n",
    "            sys.stderr.write('Error: File already exists! \\n')\n",
    "            return 0\n",
    "        tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"x:bz2\")\n",
    "        tar = tarfile.open(name = \"./data/\"+Set+\".tar.bz2\", mode = \"w:bz2\")\n",
    "        gen_set(int(N), n, x, noise, \"temp_\"+str(label_counter)+\".sublibrary\", verbosity)\n",
    "        tar.add(\"./data/temp_\"+str(label_counter)+\".sublibrary.\")\n",
    "        os.remove(\"./data/temp_\"+str(label_counter)+\".sublibrary.\")\n",
    "        end = time.time()\n",
    "        if (verbosity > 0):\n",
    "            print(\"+++++++++++++\")\n",
    "            print(\"Time for generating library: %3.2fs\" % (end-start))\n",
    "            print(\"+++++++++++++\")\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def gen_set(N, n, x, noise, Set, verbosity): \n",
    "    \"\"\"\n",
    "    - Generates a set with N spectra by using the superposition of TWO Voigt profiles with randomly choosen\n",
    "        parameters \n",
    "            gamma1: HWHM of Lorentzian part of Voigt profile 1 \n",
    "            gamma2: HWHM of Lorentzian part of Voigt profile 2\n",
    "            sigma1: Standard uncertainty of Gaussian part of Voigt profile 1\n",
    "            sigma2: Standard uncertainty of Gaussian part of Voigt profile 2\n",
    "            epsilons: offset to energy E, dE, Ratios\n",
    "        The Energy E (K alpha1) is centered around 2014eV \n",
    "        Splitting is set to 0.85eV +- 0.05\n",
    "        Ratios are set to 1.7 pm 0.5\n",
    "    \"\"\"\n",
    "    if (verbosity > 0):\n",
    "        start = time.time()\n",
    "    # Definition of some parameters\n",
    "    gamma1, sigma1 = 0.345, 0.07\n",
    "    gamma2, sigma2 = 0.36, 0.08\n",
    "    \n",
    "    # Creating the empty data dictionary to store data\n",
    "    k = 5\n",
    "    X = {\n",
    "        'states': np.zeros(N),\n",
    "        'energies': np.zeros(N*k).reshape(N,k),\n",
    "        'energies_binned': np.zeros(N*k).reshape(N,k),\n",
    "        'splittings': np.zeros(N*k).reshape(N,k),\n",
    "        'ratios': np.zeros(N*k).reshape(N,k),\n",
    "        'features': np.zeros(N*len(x)).reshape(N,len(x))\n",
    "    }\n",
    "    \n",
    "    runtime = np.array(0)\n",
    "    runtime = np.delete(runtime, 0)\n",
    "    \"\"\"\n",
    "    For loop loops N times to create N spectra. The single spectrum is evaluate and fitted\n",
    "    on range x to get equal x values as features (Note: When trained on grid defined by x then\n",
    "    real data must also be sampled on same grid!). File format:\n",
    "        File dimensions: N x (2 + d), where d is number of grid points resulting from grid x\n",
    "        [E dE x1 x2 ... xd]\n",
    "    \"\"\"\n",
    "    if (verbosity > 0):\n",
    "        loop_time_start = time.time()\n",
    "    for i in range(N):\n",
    "        # Create empty feature array\n",
    "        feature_array = np.zeros(len(x))\n",
    "        ## Generate superposition of n different oxidation station \n",
    "        # Save number of states in dictionary\n",
    "        X[\"states\"][i] = n\n",
    "        for j in range(n):\n",
    "            # Generate random distribution (+- 1) around central value of energie E\n",
    "            E_epsilon = (np.random.random_sample()-0.5)*2\n",
    "            # Generate random distribution (+- 0.1) around central value of energie dE\n",
    "            dE_epsilon = (np.random.random_sample()-0.5)*(0.4)\n",
    "            # Generate random distribution (+- 0.05) around central value of amplitude L1\n",
    "            dL1 = (np.random.random_sample()-0.5)/15\n",
    "            # Generate random distribution (+- 0.05) around central value of amplitude L2\n",
    "            dL2 = (np.random.random_sample()-0.5)/15\n",
    "            L1 = 0.18 + dL1\n",
    "            L2 = 0.3 + dL2\n",
    "            E = 2014 + E_epsilon\n",
    "            dE = 0.85 + dE_epsilon\n",
    "            v1 = Voigt1D(x_0=E-dE, amplitude_L=L1, fwhm_L=2*gamma1, fwhm_G=2*sigma1*np.sqrt(2*np.log(2)))\n",
    "            v2 = Voigt1D(x_0=E, amplitude_L=L2, fwhm_L=2*gamma2, fwhm_G=2*sigma2*np.sqrt(2*np.log(2)))\n",
    "            # Superpose the states\n",
    "            feature_array += v1(x)+v2(x)\n",
    "            # Calculate the ratio of the areas\n",
    "            R = calc_area(x, v2(x))/calc_area(x,v1(x))\n",
    "            ### Discretize Energies for classification option\n",
    "            # Define energy bins\n",
    "            bins = np.arange(2013,2015+0.01,0.01)\n",
    "            # Apply this grid on enegery and center energy to the bin center to increase precision\n",
    "            binned = np.digitize(E, bins, right=True)\n",
    "            E_binned = bins[binned]#-0.005\n",
    "            # Save values in dictionary\n",
    "            X[\"energies\"][i,j+0] = E\n",
    "            X[\"energies_binned\"][i,j] = E_binned\n",
    "            X[\"splittings\"][i,j] = dE\n",
    "            X[\"ratios\"][i,j] = R\n",
    "        ## Apply noise to data    \n",
    "        if (noise == True):\n",
    "            # Normalize spectrum to 1\n",
    "            amp = np.amax(feature_array)\n",
    "            feature_array  /= amp\n",
    "            # Apply poisson noise to the data, magnify amplitudes to get poisson function working\n",
    "            feature_array =np.multiply(feature_array,0.5*1e3)\n",
    "            feature_array = np.random.poisson(feature_array)\n",
    "            # Scale down again\n",
    "            feature_array = np.divide(feature_array, 0.5*1e4)\n",
    "            # Fill ddictionary:\n",
    "            X[\"features\"][i] = feature_array\n",
    "        else: \n",
    "            # Normalize spectrum to 1\n",
    "            amp = np.amax(feature_array)\n",
    "            feature_array  /= amp\n",
    "            # Fill ddictionary:\n",
    "            X[\"features\"][i] = feature_array\n",
    "        # Runtime control\n",
    "        if (verbosity > 0):\n",
    "            if ( i % (N/10) == 0 ):\n",
    "                loop_time_end = time.time()\n",
    "                time_diff = loop_time_end-loop_time_start\n",
    "                runtime = np.append(runtime, time_diff)\n",
    "                print(\"Progress: %i/%i, time for loop: %3.2fs\" % (i , N, time_diff))\n",
    "                loop_time_start = time.time()\n",
    "    # Save dictinoary as library in .msgpack file\n",
    "    with open('./data/'+Set,'wb') as f:\n",
    "        dill.dump(X,f)\n",
    "    # Some verbosity output\n",
    "    if (verbosity > 0):\n",
    "        end = time.time()\n",
    "        print(\"Time for generating the \"+Set+\" set:\", end-start)\n",
    "        if (verbosity > 1):\n",
    "            plt.figure()\n",
    "            plt.title(\"Runtime for generating the data set \"+Set)\n",
    "            plt.plot(runtime, label=\"Runtime per N/10 loops\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plotly_show()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_library(Set, fraction, verbosity):\n",
    "    \"\"\"\n",
    "    Function wrapped around read_set() for loading the training library. \n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    tar = tarfile.open(\"./data/\"+Set+\".tar.bz2\", \"r:bz2\")\n",
    "    label_counter = 1\n",
    "    k = 0\n",
    "    N = 0\n",
    "    l = 0\n",
    "    X = {\n",
    "        'states': np.zeros(N),\n",
    "        'energies': np.zeros(N*k).reshape(N,k),\n",
    "        'energies_binned': np.zeros(N*k).reshape(N,k),\n",
    "        'splittings': np.zeros(N*k).reshape(N,k),\n",
    "        'ratios': np.zeros(N*k).reshape(N,k),\n",
    "        'features': np.zeros(N*l).reshape(N,l)\n",
    "    }\n",
    "    for i in (tar):\n",
    "        if ( label_counter > fraction ):\n",
    "            break\n",
    "        start_loop = time.time()\n",
    "        tar.extract(i)\n",
    "        temp = read_set(\"temp_\"+str(label_counter)+\".sublibrary\")\n",
    "        for item in legend:\n",
    "            X[item] = np.append(X[item], temp[item])\n",
    "        end_loop = time.time()\n",
    "        if (verbosity > 0):\n",
    "            print(\"Time for unpacking: %3.2fs\" % (end_loop-start_loop))\n",
    "        os.remove(\"./data/temp_\"+str(label_counter)+\".sublibrary\")\n",
    "        label_counter += 1\n",
    "    tar.close()\n",
    "    end = time.time()\n",
    "    if (verbosity > 0):\n",
    "        print(\"+++++++++++\")\n",
    "        print(\"Time for loading the library: %3.2fs\" % (end-start))\n",
    "        print(\"+++++++++++\")\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def read_set(Set):\n",
    "    \"\"\" \n",
    "    Read data and store it in (Nxd) Martix, where N donates \n",
    "    the observation (single spectrum) and d the dth feature \n",
    "    (datapoint given by choosing x). The data gets fitted \n",
    "    by the Splines fit. Also, noise is added when reading the\n",
    "    data if flag is set.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    with open('./data/'+Set,'rb') as f:\n",
    "        X = dill.load(f)\n",
    "    end = time.time()\n",
    "    print(\"Time for reading \"+Set+\" set: %3.2fs\" % (end-start))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale_input(X):\n",
    "    \"\"\"\n",
    "    Feature skaling for NN apporach. It is \"highly recommended\" to scale input data to either [0:1] or [-1:+1] \n",
    "    or standardize it to have mean 0 and variance 1\n",
    "    Source:\n",
    "    http://scikit-learn.org/stable/modules/neural_networks_supervised.html#regression\n",
    "    This function standardizes X \n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler  \n",
    "    scaler = StandardScaler()  \n",
    "    # Don't cheat - fit only on training data\n",
    "    N = len(X)\n",
    "    d = len(X[0]+n_labels)\n",
    "    scaler.fit(X)  \n",
    "    X = scaler.transform(X) \n",
    "    #for i in range(len(X)):\n",
    "    #    plt.plot(x,X[i])\n",
    "    #plt.grid(True)\n",
    "    #plotly_show()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NN_train(library, model, parameter, scaling, verbosity):\n",
    "    \"\"\"\n",
    "    Trains given model on data X and labels y. Returns trainings score\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    Xtrain, Ytrain, Xdev, Ydev, Xtest, Ytest = split_library(library)\n",
    "    X = Xtrain\n",
    "    y = np.ravel(Ytrain[:,parameter])*1e3 # multiply to create an integer problem\n",
    "    y = y.astype(int)\n",
    "    if (scaling == True):\n",
    "        X = scale_input(X)\n",
    "    # Set out pipe to catch stdout for getting verbosity output of model.fit\n",
    "    if (verbosity > 0):\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = mystdout = io.StringIO()\n",
    "    model.fit(X, y)\n",
    "    # Delete pipe\n",
    "    if (verbosity > 0):\n",
    "        sys.stdout = old_stdout\n",
    "    # Save verbosity output (training loss) in variable\n",
    "    loss = np.array(0)\n",
    "    loss = np.delete(loss, 0)\n",
    "    if (verbosity > 0):\n",
    "        verbosity_output = mystdout.getvalue()\n",
    "        verbosity_output = np.array(verbosity_output.split(' '))\n",
    "        for i in range(4,len(verbosity_output),4):\n",
    "            if (verbosity_output[i].split('\\n')[0] == 'improve'):\n",
    "                break\n",
    "            else:\n",
    "                loss = np.append(loss, float(verbosity_output[i].split('\\n')[0]))\n",
    "    # Save score of training\n",
    "    score = model.score(X,y)\n",
    "    end = time.time()\n",
    "    # Print training statistics depending onb verbosity level\n",
    "    if (verbosity > 0):\n",
    "        print(\"Training time: %3.2f \" % (end - start))\n",
    "        print(\"Training score: %3.2f \" % (score))\n",
    "        if (verbosity > 1):\n",
    "            plt.figure()\n",
    "            plt.title(\"Training loss per epoch\")\n",
    "            plt.semilogy(loss, label=\"Loss\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plotly_show()       \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_envir(X_train, Y_train, X_dev, Y_dev, X_test, Y_test, model, param, verbosity, scaling):\n",
    "    \"\"\"\n",
    "    Trains and tests a NN on a given label\n",
    "    \"\"\"\n",
    "    param = 0\n",
    "    score = NN_train(X_train, Y_train, model, param, scaling, verbosity = verbosity)\n",
    "    #Save model via\n",
    "    joblib.dump(model, './data/1_neural_network.pkl')\n",
    "    #Load model via\n",
    "    model2 = joblib.load('./data/1_neural_network.pkl')\n",
    "    predict_train = model.predict(X_train)\n",
    "    predict_dev = model.predict(X_dev)\n",
    "    plt.plot(Y_train[:,param]-predict_train, label=(\"Train: Loss energy\"))\n",
    "    plt.plot(Y_dev[:,param]-predict_dev, label=(\"Dev: Loss energy\"))\n",
    "    plt.xlabel(\"datapoint\")\n",
    "    plt.ylabel(\"Error in arb. units\")\n",
    "    plt.title(\"Error on true label\")\n",
    "    abserr_train = np.absolute(Y_train[:,param]-predict_train)\n",
    "    abserr_train = np.sum(abserr_train)\n",
    "    abserr_dev = np.absolute(Y_dev[:,param]-predict_dev)\n",
    "    abserr_dev = np.sum(abserr_dev)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    print(\"Mean error per prediction in training set in run %3.2f \" % (abserr_train/len(X_train)))\n",
    "    print(\"Mean error per prediction in dev set in run %3.2f \" % (abserr_dev/len(X_dev)))\n",
    "    plotly_show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def split_library(library, key):\n",
    "    \"\"\"\n",
    "    Function to split a library in training, dev, and test set after good 'ol ratio 80%:10%:10%\n",
    "    \"\"\"\n",
    "    N = len(library[\"states\"])\n",
    "    N_train = int(N*0.8)\n",
    "    N_dev = N_test = int(N*0.1)\n",
    "    Xtrain = library[\"features\"][0:N_train]\n",
    "    Ytrain = library[key][0:N_train]\n",
    "    Xdev = library[\"features\"][N_train : N_train + N_dev]\n",
    "    Ydev = library[key][N_train : N_train + N_dev]\n",
    "    Xtest = library[\"features\"][N_train + N_dev : N_train + 2*N_dev]\n",
    "    Ytest = library[key][N_train + N_dev : N_train + 2*N_dev]\n",
    "    return Xtrain, Ytrain, Xdev, Ydev, Xtest, Ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin of testing the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Defining (Hyper)Parameters\n",
    "\"\"\"\n",
    "exp = 3 # Exponent defining the size of the file\n",
    "factor = 1\n",
    "N = int(factor*10**(exp)) # Actual value\n",
    "noisee = True\n",
    "n = 5 # Number of spectra\n",
    "comment = \"test\" # Comment for data file name\n",
    "data_size = str(int(N/1000))+\"k_\" # Value for labeling the data (in \"kilo samples\") \n",
    "set_name = data_size+comment+\"_library\"\n",
    "x = np.arange(2008,2018,0.05) # Grid for creating and importing data\n",
    "# Following the definition of the different \n",
    "modelR_energy = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelR_splitting = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelR_ratios = MLPR(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelC_energy = MLPC(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelC_splitting = MLPC(max_iter=2000,  activation=\"relu\", verbose = True)\n",
    "modelC_ratios = MLPC(max_iter=2000,  activation=\"relu\", verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "build_library(N, n, x, noise = True, Set = \"test\", verbosity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library = load_library(Set = \"test\", fraction = 1000, verbosity=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
